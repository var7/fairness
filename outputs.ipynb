{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## sys imports #############\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import datetime\n",
    "############## basic stats imports #############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "############## pytorch imports #############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## custom imports #############\n",
    "from dataloader import FaceScrubDataset, TripletFaceScrub, SiameseFaceScrub\n",
    "from dataloader import FaceScrubBalancedBatchSampler\n",
    "\n",
    "from networks import *\n",
    "from losses import OnlineTripletLoss\n",
    "from openface.loadOpenFace import prepareOpenFace\n",
    "from utils import save_checkpoint, save_hyperparams, AverageMeter, HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/s1791387/facescrub-data/new_data_max/'\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train_full_with_ids.txt')\n",
    "VALID_PATH = os.path.join(DATA_PATH, 'val_full_with_ids.txt')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test_full_with_ids.txt')\n",
    "WEIGHTS_PATH = '/home/s1791387/facescrub-data/new_data_max/openface_model_weigths/job_semi_std_cos3_Jul_25_1000hrs/weights_75.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "input_size = 96\n",
    "output_dim = 128\n",
    "learning_rate = 1e2\n",
    "num_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "triplet_margin = 1.  # margin\n",
    "triplet_p = 2  # norm degree for distance calculation\n",
    "\n",
    "resume_training = True\n",
    "workers = 8\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set: cuda\n",
      "Training set path: /home/s1791387/facescrub-data/new_data_max/train_full_with_ids.txt\n",
      "Training set Path exists: True\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "pin_memory = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    pin_memory = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Device set: {}'.format(device))\n",
    "print('Training set path: {}'.format(TRAIN_PATH))\n",
    "print('Training set Path exists: {}'.format(os.path.isfile(TRAIN_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data converted to siamese form. Length: 54981\n",
      "Validation data converted to siamese form. Length: 5881\n",
      "Train loader created. Length of train loader: 108\n",
      "Val loader created. Length of train loader: 12\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "}\n",
    "\n",
    "\n",
    "train_df = FaceScrubDataset(\n",
    "    txt_file=TRAIN_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "val_df = FaceScrubDataset(\n",
    "    txt_file=VALID_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "siamese_train_df = SiameseFaceScrub(train_df, train=True)\n",
    "print('Train data converted to siamese form. Length: {}'.format(len(siamese_train_df)))\n",
    "\n",
    "siamese_val_df=SiameseFaceScrub(val_df, train=False)\n",
    "print('Validation data converted to siamese form. Length: {}'.format(\n",
    "    len(siamese_val_df)))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(\n",
    "        siamese_train_df, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Train loader created. Length of train loader: {}'.format(\n",
    "        len(train_loader)))\n",
    "    \n",
    "val_loader=torch.utils.data.DataLoader(\n",
    "        siamese_val_df, batch_size=batch_size, shuffle=False, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Val loader created. Length of train loader: {}'.format(\n",
    "        len(val_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent model to GPU\n",
      "Number of params in network 3733968\n"
     ]
    }
   ],
   "source": [
    "openface = prepareOpenFace(useCuda=cuda)\n",
    "params = sum(p.numel() for p in openface.parameters() if p.requires_grad)\n",
    "print('Number of params in network {}'.format(params))\n",
    "\n",
    "en_optimizer=optim.Adam(openface.parameters(), lr=learning_rate)\n",
    "\n",
    "T_max = num_epochs\n",
    "eta_min = 0.01\n",
    "en_scheduler = lr_scheduler.CosineAnnealingLR(en_optimizer, T_max=T_max, eta_min=eta_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassNet(input_size=output_dim, training=True)\n",
    "cl_optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "T_max = num_epochs\n",
    "eta_min = 0.01\n",
    "cl_scheduler = lr_scheduler.CosineAnnealingLR(cl_optimizer, T_max=T_max, eta_min=eta_min)\n",
    "cl_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint '/home/s1791387/facescrub-data/new_data_max/openface_model_weigths/job_semi_std_cos3_Jul_25_1000hrs/weights_75.pth' (trained for 75 epochs)\n"
     ]
    }
   ],
   "source": [
    "if resume_training:\n",
    "    resume_weights=WEIGHTS_PATH\n",
    "    if cuda:\n",
    "        checkpoint=torch.load(resume_weights)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint=torch.load(resume_weights,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "\n",
    "    start_epoch=checkpoint['epoch']\n",
    "    openface.load_state_dict(checkpoint['state_dict'])\n",
    "    en_optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(\n",
    "        resume_weights, checkpoint['epoch']))\n",
    "#     for epoch in range(0, start_epoch):\n",
    "#         en_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent model to gpu True\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    openface.cuda()\n",
    "    classifier.cuda()\n",
    "    print('Sent model to gpu {}'.format(\n",
    "        next(openface.parameters()).is_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, classifier, encoder, criterion, en_optimizer, cl_optimizer, epoch, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    print_freq=1\n",
    "    # switch to train mode\n",
    "    classifier.train()\n",
    "    encoder.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, ([imgs1,imgs2], [labels1, labels2], target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        imgs1 = imgs1.to(device)\n",
    "        imgs2 = imgs2.to(device)\n",
    "        target = target.to(device).float()\n",
    "#         print(target.shape, target)\n",
    "        embed1, _ = encoder(imgs1)\n",
    "        embed2, _ = encoder(imgs2)\n",
    "        pair_embed = torch.cat((embed1, embed2), dim=1)\n",
    "#         print(pair_embed.shape)\n",
    "        pred_target = classifier(pair_embed)\n",
    "        pred_target.squeeze_()\n",
    "#         print(pred_target.squeeze_())\n",
    "#         print(pred_target.shape)\n",
    "        loss = cl_criterion(pred_target, target)\n",
    "#         print(loss)\n",
    "        losses.update(loss.item(), imgs1[0].size(0))\n",
    "\n",
    "        en_optimizer.zero_grad()\n",
    "        cl_optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        en_optimizer.step()\n",
    "        cl_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch, batch_idx, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, classifier, encoder, criterion, epoch, device):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    print_freq=100\n",
    "    # switch to evaluate mode\n",
    "    classifier.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for batch_idx, ([imgs1,imgs2], [labels1, labels2], target) in enumerate(val_loader):\n",
    "            imgs1 = imgs1.to(device)\n",
    "            imgs2 = imgs2.to(device)\n",
    "            target = target.to(device).float()\n",
    "    #         print(target.shape, target)\n",
    "            embed1, _ = openface(imgs1)\n",
    "            embed2, _ = openface(imgs2)\n",
    "            pair_embed = torch.cat((embed1, embed2), dim=1)\n",
    "    #         print(pair_embed.shape)\n",
    "            pred_target = classifier(pair_embed)\n",
    "            pred_target.squeeze_()\n",
    "    #         print(pred_target.squeeze_())\n",
    "    #         print(pred_target.shape)\n",
    "            loss = cl_criterion(pred_target, target)\n",
    "    #         print(loss)\n",
    "            losses.update(loss.item(), imgs1[0].size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                       batch_idx, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Beginning Training\n",
      "Epoch: [75][0/108]\tTime 89.851 (89.851)\tData 65.511 (65.511)\tLoss 0.6938 (0.6938)\n",
      "Epoch: [75][1/108]\tTime 0.695 (45.273)\tData 0.005 (32.758)\tLoss 68295.7734 (34148.2336)\n",
      "Epoch: [75][2/108]\tTime 0.711 (30.419)\tData 0.006 (21.840)\tLoss 567761.1250 (212019.1974)\n",
      "Epoch: [75][3/108]\tTime 0.694 (22.988)\tData 0.001 (16.381)\tLoss 2755523.0000 (847895.1481)\n",
      "Epoch: [75][4/108]\tTime 22.062 (22.803)\tData 21.374 (17.379)\tLoss 519568.5000 (782229.8184)\n",
      "Epoch: [75][5/108]\tTime 0.706 (19.120)\tData 0.001 (14.483)\tLoss 2924359.7500 (1139251.4737)\n",
      "Epoch: [75][6/108]\tTime 0.695 (16.488)\tData 0.001 (12.414)\tLoss 1538636.2500 (1196306.4417)\n",
      "Epoch: [75][7/108]\tTime 0.711 (14.516)\tData 0.001 (10.862)\tLoss 729353.3125 (1137937.3006)\n",
      "Epoch: [75][8/108]\tTime 52.591 (18.746)\tData 51.804 (15.411)\tLoss 135825.2188 (1026591.5137)\n",
      "Epoch: [75][9/108]\tTime 0.806 (16.952)\tData 0.001 (13.870)\tLoss 139603.5938 (937892.7217)\n",
      "Epoch: [75][10/108]\tTime 0.792 (15.483)\tData 0.001 (12.610)\tLoss 112430.2188 (862850.6760)\n",
      "Epoch: [75][11/108]\tTime 0.807 (14.260)\tData 0.001 (11.559)\tLoss 30435.8535 (793482.7741)\n",
      "Epoch: [75][12/108]\tTime 46.768 (16.761)\tData 46.057 (14.213)\tLoss 75212.1484 (738231.1875)\n",
      "Epoch: [75][13/108]\tTime 0.694 (15.613)\tData 0.020 (13.199)\tLoss 97859.7891 (692490.3734)\n",
      "Epoch: [75][14/108]\tTime 0.707 (14.619)\tData 0.030 (12.321)\tLoss 44699.2461 (649304.2982)\n",
      "Epoch: [75][15/108]\tTime 0.751 (13.753)\tData 0.001 (11.551)\tLoss 10993.3086 (609409.8614)\n",
      "Epoch: [75][16/108]\tTime 54.693 (16.161)\tData 53.974 (14.046)\tLoss 46.9595 (573564.9848)\n",
      "Epoch: [75][17/108]\tTime 0.692 (15.302)\tData 0.001 (13.266)\tLoss 54.1304 (541703.2706)\n",
      "Epoch: [75][18/108]\tTime 0.722 (14.534)\tData 0.001 (12.568)\tLoss 54.5029 (513195.4408)\n",
      "Epoch: [75][19/108]\tTime 0.705 (13.843)\tData 0.003 (11.940)\tLoss 51.1288 (487538.2252)\n",
      "Epoch: [75][20/108]\tTime 42.344 (15.200)\tData 41.634 (13.354)\tLoss 40.4482 (464324.0453)\n",
      "Epoch: [75][21/108]\tTime 1.319 (14.569)\tData 0.646 (12.776)\tLoss 23.2691 (443219.4646)\n",
      "Epoch: [75][22/108]\tTime 0.702 (13.966)\tData 0.014 (12.221)\tLoss 2.2116 (423949.1492)\n",
      "Epoch: [75][23/108]\tTime 0.718 (13.414)\tData 0.001 (11.712)\tLoss 23.8388 (406285.5946)\n",
      "Epoch: [75][24/108]\tTime 42.237 (14.567)\tData 41.377 (12.899)\tLoss 41.8560 (390035.8451)\n",
      "Epoch: [75][25/108]\tTime 0.791 (14.037)\tData 0.004 (12.403)\tLoss 48.3426 (375036.3258)\n",
      "Epoch: [75][26/108]\tTime 2.225 (13.600)\tData 1.460 (11.997)\tLoss 49.1749 (361147.9128)\n",
      "Epoch: [75][27/108]\tTime 0.795 (13.142)\tData 0.001 (11.569)\tLoss 42.2369 (348251.2815)\n",
      "Epoch: [75][28/108]\tTime 45.656 (14.264)\tData 44.922 (12.719)\tLoss 25.3232 (336243.4898)\n",
      "Epoch: [75][29/108]\tTime 0.694 (13.811)\tData 0.005 (12.295)\tLoss 8.2526 (325035.6486)\n",
      "Epoch: [75][30/108]\tTime 5.668 (13.549)\tData 4.989 (12.060)\tLoss 13.2907 (314551.0564)\n",
      "Epoch: [75][31/108]\tTime 0.694 (13.147)\tData 0.004 (11.683)\tLoss 25.9104 (304722.1456)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-3:\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 135, in __getitem__\n",
      "    img1, labels1 = self.facescrub_dataset[index]\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 146, in __getitem__\n",
      "    img2, labels2 = self.facescrub_dataset[siamese_index]\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 135, in __getitem__\n",
      "    img1, labels1 = self.facescrub_dataset[index]\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 60, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 60, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 60, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/PIL/Image.py\", line 2548, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/PIL/Image.py\", line 2548, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/PIL/Image.py\", line 2548, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 135, in __getitem__\n",
      "    img1, labels1 = self.facescrub_dataset[index]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/mscteach_home/s1791387/diss/dataloader.py\", line 60, in __getitem__\n",
      "    img = Image.open(img_path)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/PIL/Image.py\", line 2548, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-3197ebd222c4>\", line 13, in <module>\n",
      "    train_loss = train(train_loader, classifier, openface, cl_criterion, en_optimizer, cl_optimizer, epoch, device)\n",
      "  File \"<ipython-input-12-0cd535ed7af7>\", line 11, in train\n",
      "    for batch_idx, ([imgs1,imgs2], [labels1, labels2], target) in enumerate(train_loader):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 330, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 309, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "  File \"/home/s1791387/miniconda3/envs/fairness/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 14829) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print('-'*10)\n",
    "print('Beginning Training')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epoch_time = AverageMeter()\n",
    "ep_end = time.time()\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "\n",
    "    en_scheduler.step()\n",
    "    cl_scheduler.step()\n",
    "\n",
    "    # train\n",
    "    train_loss = train(train_loader, classifier, openface, cl_criterion, en_optimizer, cl_optimizer, epoch, device)\n",
    "    train_losses.append(train_loss)\n",
    "    # validate\n",
    "    print('-'*10)\n",
    "    val_loss = validate(val_loader, classifier, openface, cl_criterion, epoch, device)\n",
    "\n",
    "    print('Avg validation loss: {}'.format(val_loss))\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': openface.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_loss': best_loss\n",
    "        # 'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    if best_loss > val_loss:\n",
    "        best_loss = val_loss\n",
    "        MODEL_NAME = os.path.join(\n",
    "            WEIGHTS_PATH, 'weights_{}.pth'.format(epoch))\n",
    "        save_checkpoint(state, True, WEIGHTS_PATH, MODEL_NAME)\n",
    "    print('-' * 20)\n",
    "    epoch_time.update(time.time() - ep_end)\n",
    "    ep_end = time.time()\n",
    "    print('Epoch {}/{}\\t'\n",
    "          'Time {epoch_time.val:.3f} sec ({epoch_time.avg:.3f} sec)'.format(epoch, start_epoch + num_epochs - 1, epoch_time=epoch_time))\n",
    "    print('-'*20)\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(embeddings):\n",
    "    import sklearn.manifold\n",
    "    return torch.from_numpy(sklearn.manifold.TSNE(n_iter = 250).fit_transform(embeddings.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg(points, labels, thumbnails, legend_size = 1e-1, legend_font_size = 5e-2, circle_radius = 5e-3):\n",
    "\tpoints = (points - points.min(0)[0]) / (points.max(0)[0] - points.min(0)[0])\n",
    "\tclass_index = sorted(set(labels))\n",
    "\tclass_colors = [360.0 * i / len(class_index) for i in range(len(class_index))]\n",
    "\tcolors = [class_colors[class_index.index(label)] for label in labels]\n",
    "\tthumbnails_base64 = [base64.b64encode(cv2.imencode('.jpg', img.mul(255).permute(1, 2, 0).numpy()[..., ::-1])[1]) for img in thumbnails]\n",
    "\treturn '<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1 1\">' + \\\n",
    "\t   ''.join(map('''<circle cx=\"{}\" cy=\"{}\" title=\"{}\" fill=\"hsl({}, 50%, 50%)\" r=\"{}\" desc=\"data:image/jpeg;base64,{}\" onmouseover=\"evt.target.ownerDocument.getElementById('preview').setAttribute('href', evt.target.getAttribute('desc')); evt.target.ownerDocument.getElementById('label').textContent = evt.target.getAttribute('title');\" />'''.format, points[:, 0], points[:, 1], labels, colors, [circle_radius] * len(points), thumbnails_base64)) + \\\n",
    "\t   '''<image id=\"preview\" x=\"0\" y=\"{legend_size}\" width=\"{legend_size}\" height=\"{legend_size}\" />\n",
    "\t   <text id=\"label\" x=\"0\" y=\"{legend_size}\" font-size=\"{legend_font_size}\" />\n",
    "\t   </svg>'''.format(legend_size = legend_size, legend_font_size = legend_font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embeddings = tsne(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('train_tsne.svg', 'w').write(svg(tsne_embeddings, person_id, thumbnails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.numpy()\n",
    "np.correlate(train_embeddings, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mutual_info_score(train_embeddings, gender, contingency=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
