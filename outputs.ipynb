{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## sys imports #############\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import datetime\n",
    "############## basic stats imports #############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "############## pytorch imports #############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "############## custom imports #############\n",
    "from dataloader import FaceScrubDataset, TripletFaceScrub, SiameseFaceScrub\n",
    "from dataloader import FaceScrubBalancedBatchSampler\n",
    "\n",
    "from networks import *\n",
    "from losses import OnlineTripletLoss\n",
    "\n",
    "from utils import save_checkpoint, save_hyperparams, AverageMeter, HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './new_data/'\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train_full_with_ids.txt')\n",
    "VALID_PATH = os.path.join(DATA_PATH, 'val_full_with_ids.txt')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test_full_with_ids.txt')\n",
    "WEIGHTS_PATH = './balanced_weights/weights_43.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "input_size = 299\n",
    "output_dim = 128\n",
    "learning_rate = 1e2\n",
    "num_epochs = 1\n",
    "start_epoch = 0\n",
    "\n",
    "triplet_margin = 1.  # margin\n",
    "triplet_p = 2  # norm degree for distance calculation\n",
    "\n",
    "resume_training = True\n",
    "workers = 4\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set: cpu\n",
      "Training set path: ./new_data/train_full_with_ids.txt\n",
      "Training set Path exists: True\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "pin_memory = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    pin_memory = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Device set: {}'.format(device))\n",
    "print('Training set path: {}'.format(TRAIN_PATH))\n",
    "print('Training set Path exists: {}'.format(os.path.isfile(TRAIN_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader created. Length of train loader: 6879\n",
      "Val loader created. Length of train loader: 736\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "}\n",
    "\n",
    "\n",
    "train_df = FaceScrubDataset(\n",
    "    txt_file=TRAIN_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "val_df = FaceScrubDataset(\n",
    "    txt_file=VALID_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(\n",
    "        train_df, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Train loader created. Length of train loader: {}'.format(\n",
    "        len(train_loader)))\n",
    "    \n",
    "val_loader=torch.utils.data.DataLoader(\n",
    "        val_df, batch_size=batch_size, shuffle=False, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Val loader created. Length of train loader: {}'.format(\n",
    "        len(val_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded from ./new_data/train_full_with_ids.txt. Length: 55029\n",
      "Validation data loaded from ./new_data/val_full_with_ids.txt. Length: 5888\n",
      "Online train loader created. Length: 1100\n"
     ]
    }
   ],
   "source": [
    "train_batch_sampler = FaceScrubBalancedBatchSampler(\n",
    "        train_df, n_classes=5, n_samples=10)\n",
    "val_batch_sampler = FaceScrubBalancedBatchSampler(\n",
    "    val_df, n_classes=5, n_samples=10)\n",
    "\n",
    "print('Train data loaded from {}. Length: {}'.format(\n",
    "    TRAIN_PATH, len(train_df)))\n",
    "print('Validation data loaded from {}. Length: {}'.format(\n",
    "    VALID_PATH, len(val_df)))\n",
    "\n",
    "online_train_loader = torch.utils.data.DataLoader(\n",
    "    train_df, batch_sampler=train_batch_sampler, pin_memory=pin_memory, num_workers=workers)\n",
    "\n",
    "print('Online train loader created. Length: {}'.format(\n",
    "    len(online_train_loader)))\n",
    "\n",
    "online_val_loader = torch.utils.data.DataLoader(\n",
    "    val_df, batch_sampler=val_batch_sampler, pin_memory=pin_memory, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception=models.inception_v3(pretrained=True)\n",
    "inception.aux_logits=False\n",
    "num_ftrs=inception.fc.in_features\n",
    "inception.fc=nn.Linear(num_ftrs, output_dim)\n",
    "\n",
    "criterion=nn.TripletMarginLoss(margin=triplet_margin, p=triplet_p)\n",
    "\n",
    "optimizer=optim.Adam(inception.parameters(), lr=learning_rate)\n",
    "scheduler=lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint './balanced_weights/weights_43.pth' (trained for 43 epochs)\n"
     ]
    }
   ],
   "source": [
    "if resume_training:\n",
    "    resume_weights=WEIGHTS_PATH\n",
    "    if cuda:\n",
    "        checkpoint=torch.load(resume_weights)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint=torch.load(resume_weights,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "\n",
    "    start_epoch=checkpoint['epoch']\n",
    "    inception.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(\n",
    "        resume_weights, checkpoint['epoch']))\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    tripletinception.cuda()\n",
    "    print('Sent model to gpu {}'.format(\n",
    "        next(tripletinception.parameters()).is_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (imgs, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "\n",
    "        embed_anchor, embed_pos, embed_neg=model(imgs[0], imgs[1], imgs[2])\n",
    "        loss = criterion(embed_anchor, embed_pos, embed_neg)\n",
    "\n",
    "        losses.update(loss.item(), imgs[0].size(0))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch, batch_idx, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (imgs, _) in enumerate(val_loader):\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "\n",
    "            embed_anchor, embed_pos, embed_neg=model(imgs[0], imgs[1], imgs[2])\n",
    "\n",
    "            loss = criterion(embed_anchor, embed_pos, embed_neg)\n",
    "\n",
    "            losses.update(loss.item(), imgs[0].size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [10, 5, 279, 330]\n",
    "imgs = []\n",
    "person_id = []\n",
    "gender = []\n",
    "count = 1\n",
    "for ind, row in train_df.faces_frame.iterrows():\n",
    "    if row['person_id'] in classes:\n",
    "        count += 1\n",
    "        img, label = train_df[ind]\n",
    "        img = img.to(device)\n",
    "        img.unsqueeze_(0)\n",
    "        imgs.append(img)\n",
    "        person_id.append(row['person_id'])\n",
    "#         embedding = inception(img)\n",
    "#         train_embeddings = torch.cat((train_embeddings, embedding))\n",
    "#     if count / 10 == 0: print(count)\n",
    "#         thumbnails = torch.cat((thumbnails, img))\n",
    "#         person_id = np.concatenate((person_id, label['person_id']))\n",
    "#         gender = np.concatenate((gender, label['gender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images completed\n",
      "20 images completed\n",
      "40 images completed\n",
      "60 images completed\n",
      "80 images completed\n",
      "100 images completed\n",
      "120 images completed\n",
      "140 images completed\n",
      "160 images completed\n",
      "180 images completed\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = torch.Tensor()\n",
    "thumbnails = torch.Tensor()\n",
    "\n",
    "for ind, img in enumerate(imgs):\n",
    "\n",
    "    embedding = inception(img)\n",
    "    train_embeddings = torch.cat((train_embeddings, embedding))\n",
    "    thumbnails = torch.cat((thumbnails, img))\n",
    "    if ind % 20 == 0: \n",
    "        print('{} images completed'.format(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inception.eval()\n",
    "train_embeddings = torch.Tensor()\n",
    "thumbnails = torch.Tensor()\n",
    "person_id = []\n",
    "gender = []\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(online_train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        embeddings = inception(imgs)\n",
    "        train_embeddings = torch.cat((train_embeddings, embeddings))\n",
    "        thumbnails = torch.cat((thumbnails, imgs))\n",
    "        person_id = np.concatenate((person_id, labels['person_id']))\n",
    "        gender = np.concatenate((gender, labels['gender']))\n",
    "        \n",
    "        if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(embeddings):\n",
    "    import sklearn.manifold\n",
    "    return torch.from_numpy(sklearn.manifold.TSNE(n_iter = 250).fit_transform(embeddings.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg(points, labels, thumbnails, legend_size = 1e-1, legend_font_size = 5e-2, circle_radius = 5e-3):\n",
    "\tpoints = (points - points.min(0)[0]) / (points.max(0)[0] - points.min(0)[0])\n",
    "\tclass_index = sorted(set(labels))\n",
    "\tclass_colors = [360.0 * i / len(class_index) for i in range(len(class_index))]\n",
    "\tcolors = [class_colors[class_index.index(label)] for label in labels]\n",
    "\tthumbnails_base64 = [base64.b64encode(cv2.imencode('.jpg', img.mul(255).permute(1, 2, 0).numpy()[..., ::-1])[1]) for img in thumbnails]\n",
    "\treturn '<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1 1\">' + \\\n",
    "\t   ''.join(map('''<circle cx=\"{}\" cy=\"{}\" title=\"{}\" fill=\"hsl({}, 50%, 50%)\" r=\"{}\" desc=\"data:image/jpeg;base64,{}\" onmouseover=\"evt.target.ownerDocument.getElementById('preview').setAttribute('href', evt.target.getAttribute('desc')); evt.target.ownerDocument.getElementById('label').textContent = evt.target.getAttribute('title');\" />'''.format, points[:, 0], points[:, 1], labels, colors, [circle_radius] * len(points), thumbnails_base64)) + \\\n",
    "\t   '''<image id=\"preview\" x=\"0\" y=\"{legend_size}\" width=\"{legend_size}\" height=\"{legend_size}\" />\n",
    "\t   <text id=\"label\" x=\"0\" y=\"{legend_size}\" font-size=\"{legend_font_size}\" />\n",
    "\t   </svg>'''.format(legend_size = legend_size, legend_font_size = legend_font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embeddings = tsne(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('train_tsne.svg', 'w').write(svg(tsne_embeddings, person_id, thumbnails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.numpy()\n",
    "np.correlate(train_embeddings, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mutual_info_score(train_embeddings, gender, contingency=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
