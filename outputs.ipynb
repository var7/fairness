{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## sys imports #############\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import datetime\n",
    "############## basic stats imports #############\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "############## pytorch imports #############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "############## custom imports #############\n",
    "from dataloader import FaceScrubDataset, TripletFaceScrub, SiameseFaceScrub\n",
    "from dataloader import FaceScrubBalancedBatchSampler\n",
    "\n",
    "from networks import *\n",
    "from losses import OnlineTripletLoss\n",
    "from openface.loadOpenFace import prepareOpenFace\n",
    "from utils import save_checkpoint, save_hyperparams, AverageMeter, HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/s1791387/facescrub-data/new_data_max/'\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, 'train_full_with_ids.txt')\n",
    "VALID_PATH = os.path.join(DATA_PATH, 'val_full_with_ids.txt')\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test_full_with_ids.txt')\n",
    "WEIGHTS_PATH = '/home/s1791387/facescrub-data/new_data_max/openface_model_weigths/job_semi_std_cos3_Jul_25_1000hrs/weights_75.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_size = 96\n",
    "output_dim = 128\n",
    "learning_rate = 1e2\n",
    "num_epochs = 1\n",
    "start_epoch = 0\n",
    "\n",
    "triplet_margin = 1.  # margin\n",
    "triplet_p = 2  # norm degree for distance calculation\n",
    "\n",
    "resume_training = True\n",
    "workers = 4\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set: cuda\n",
      "Training set path: /home/s1791387/facescrub-data/new_data_max/train_full_with_ids.txt\n",
      "Training set Path exists: True\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "pin_memory = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    pin_memory = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('Device set: {}'.format(device))\n",
    "print('Training set path: {}'.format(TRAIN_PATH))\n",
    "print('Training set Path exists: {}'.format(os.path.isfile(TRAIN_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader created. Length of train loader: 860\n",
      "Val loader created. Length of train loader: 92\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "}\n",
    "\n",
    "\n",
    "train_df = FaceScrubDataset(\n",
    "    txt_file=TRAIN_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "val_df = FaceScrubDataset(\n",
    "    txt_file=VALID_PATH, root_dir=DATA_PATH, transform=data_transforms['val'])\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(\n",
    "        train_df, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Train loader created. Length of train loader: {}'.format(\n",
    "        len(train_loader)))\n",
    "    \n",
    "val_loader=torch.utils.data.DataLoader(\n",
    "        val_df, batch_size=batch_size, shuffle=False, pin_memory=pin_memory, num_workers=workers)\n",
    "print('Val loader created. Length of train loader: {}'.format(\n",
    "        len(val_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent model to GPU\n",
      "Number of params in network 3733968\n"
     ]
    }
   ],
   "source": [
    "openface = prepareOpenFace(useCuda=cuda)\n",
    "params = sum(p.numel() for p in openface.parameters() if p.requires_grad)\n",
    "print('Number of params in network {}'.format(params))\n",
    "# inception=models.inception_v3(pretrained=True)\n",
    "# inception.aux_logits=False\n",
    "# num_ftrs=inception.fc.in_features\n",
    "# inception.fc=nn.Linear(num_ftrs, output_dim)\n",
    "\n",
    "# criterion=nn.TripletMarginLoss(margin=triplet_margin, p=triplet_p)\n",
    "\n",
    "optimizer=optim.Adam(inception.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler=lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_classes = len(train_df.pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassNet(input_size=output_dim, output_size=num_train_classes, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-35f143a7eb75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mopenface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# scheduler.load_state_dict(checkpoint['scheduler'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "if resume_training:\n",
    "    resume_weights=WEIGHTS_PATH\n",
    "    if cuda:\n",
    "        checkpoint=torch.load(resume_weights)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint=torch.load(resume_weights,\n",
    "                                map_location=lambda storage,\n",
    "                                loc: storage)\n",
    "\n",
    "    start_epoch=checkpoint['epoch']\n",
    "    openface.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(\n",
    "        resume_weights, checkpoint['epoch']))\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    openface.cuda()\n",
    "    print('Sent model to gpu {}'.format(\n",
    "        next(openface.parameters()).is_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for batch_idx, (imgs, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "\n",
    "        embed_anchor, embed_pos, embed_neg=model(imgs[0], imgs[1], imgs[2])\n",
    "        loss = criterion(embed_anchor, embed_pos, embed_neg)\n",
    "\n",
    "        losses.update(loss.item(), imgs[0].size(0))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch, batch_idx, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (imgs, _) in enumerate(val_loader):\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "\n",
    "            embed_anchor, embed_pos, embed_neg=model(imgs[0], imgs[1], imgs[2])\n",
    "\n",
    "            loss = criterion(embed_anchor, embed_pos, embed_neg)\n",
    "\n",
    "            losses.update(loss.item(), imgs[0].size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(embeddings):\n",
    "    import sklearn.manifold\n",
    "    return torch.from_numpy(sklearn.manifold.TSNE(n_iter = 250).fit_transform(embeddings.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg(points, labels, thumbnails, legend_size = 1e-1, legend_font_size = 5e-2, circle_radius = 5e-3):\n",
    "\tpoints = (points - points.min(0)[0]) / (points.max(0)[0] - points.min(0)[0])\n",
    "\tclass_index = sorted(set(labels))\n",
    "\tclass_colors = [360.0 * i / len(class_index) for i in range(len(class_index))]\n",
    "\tcolors = [class_colors[class_index.index(label)] for label in labels]\n",
    "\tthumbnails_base64 = [base64.b64encode(cv2.imencode('.jpg', img.mul(255).permute(1, 2, 0).numpy()[..., ::-1])[1]) for img in thumbnails]\n",
    "\treturn '<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1 1\">' + \\\n",
    "\t   ''.join(map('''<circle cx=\"{}\" cy=\"{}\" title=\"{}\" fill=\"hsl({}, 50%, 50%)\" r=\"{}\" desc=\"data:image/jpeg;base64,{}\" onmouseover=\"evt.target.ownerDocument.getElementById('preview').setAttribute('href', evt.target.getAttribute('desc')); evt.target.ownerDocument.getElementById('label').textContent = evt.target.getAttribute('title');\" />'''.format, points[:, 0], points[:, 1], labels, colors, [circle_radius] * len(points), thumbnails_base64)) + \\\n",
    "\t   '''<image id=\"preview\" x=\"0\" y=\"{legend_size}\" width=\"{legend_size}\" height=\"{legend_size}\" />\n",
    "\t   <text id=\"label\" x=\"0\" y=\"{legend_size}\" font-size=\"{legend_font_size}\" />\n",
    "\t   </svg>'''.format(legend_size = legend_size, legend_font_size = legend_font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embeddings = tsne(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('train_tsne.svg', 'w').write(svg(tsne_embeddings, person_id, thumbnails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = train_embeddings.numpy()\n",
    "np.correlate(train_embeddings, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mutual_info_score(train_embeddings, gender, contingency=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
