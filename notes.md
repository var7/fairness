Losses
KL divergence between the distribution and target noise
Adv losses = bce
in the loss maximize KL divergence between the bce loss and a random/uniform distribution

l2 loss
cross entropy loss
KL divergence loss

Add noise to the shapes dataset

State what you are doing in the first chapter
No suspense

Introduction
- motivation for fair ml
- what you are adressing
- why what you are addressing
- contribution of your result - In this thesis I show that.. (not this is the question I looked at)

conclusion
- experiment summary

explain conv nets

introduction is almost same as conclusion except for future work


Mention figures or remove them
Go easy on the figures
Figures problems
C is capital when chapter is capital
in this chapter we present is small-letter

Meeting 9th Aug
Discussion of chapter 1, 2, 3
Some comments related to chapters 1 and 2:
• The results part of section 1.2 is incomplete. - be less specific more general
• An introduction to section 2.1 is incomplete.
• Figure 2.1 - I have yet to change (b). - mention
• I have changed the title of section 2.2.1
• I have merged all the various machine learning loop diagrams into figure 2.3
• Added figure 2.5 to provide an intuition about convnets
• Section 2.6 is new
